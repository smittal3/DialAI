# DialAI
DialAI is an end-to-end, modular framework designed to handle dynamic conversation flow. It‚Äôs optimized for use cases requiring efficient data processing, such as automated customer interactions, surveys, and other real-time communication applications. The project aims to provide a scalable solution with a focus on low-latency operation and reliability, making it ideal for real-time high stake environments.

## üöÄ Key Features
- **End-to-End Conversational Flow**: Seamlessly manages the entire conversation pipeline, from recording user audio to generating responses via inference and voice synthesis.
- **Dynamic Silence Detection**: Detects pauses in speech to manage conversation flow naturally. If a long pause is detected, the conversation is blocked until the user resumes speaking, ensuring smooth interaction.
- **Asynchronous & Multithreaded**: Supports real-time audio processing and response generation with optimized multithreading and asynchronous tasks, enabling natural and low-latency conversations.
- **Powered by AWS**: Leverages AWS services like Polly for TTS, Transcribe for transcription, and Bedrock for model inference.
- **Analytics**: Includes built-in analytics to monitor resource usage, thread efficiency, and overall conversational performance for continuous optimization.
- **Interrupt Anytime**: Allows users to press Enter to interrupt and reset the conversation flow, offering full control over the interaction.

## ‚öôÔ∏è Architecture
This pipeline processes audio in real-time, handles transcription, runs inference using Bedrock's Llama models, and streams the response back through Polly to generate speech. Here‚Äôs an overview of the key components:

1. **Audio Recorder**:
Captures user input in real-time, chunks the audio into manageable segments, and streams it to the transcription service.

2. **Transcription**:
Utilizes AWS Transcribe to convert speech to text in real-time, streaming transcription events asynchronously.

3. **Silence Detection**:
Monitors the audio for silent periods, identifying the end of user speech and managing the flow of conversation naturally.

4. **Bedrock Inference**:
Sends the transcribed text to AWS Bedrock for AI model inference (using Meta Llama models) and streams the generated response in real-time.

5. **Speech Generator**:
Uses AWS Polly to synthesize the AI-generated response into speech, generating and streaming it back to the user concurrently with Bedrock inference.

```
           Main Event Loop (Async)
                    |                   
                    | - - - - - - - >|  Audio Recording Thread üé§
                    |                | 
                    |                |
                    |  (Stream data) |
                    | < - - - - - - -|
            Silence Detection ü§ê     |        
                    |                |
                    ‚ñº                |
              AWS Transcribe üìù      |
                    |                |
      (Stream Data) |<---------------|  Terminate Thread
                    ‚ñº                |
             Accumulate User Input   |
                    |                |
                    |                |  
                    ‚ñº
                 Bedrock üíª
                    |   
                    |   Stream data
                    | - - - - - - - >|  Speech Synthesis Thread üó£Ô∏è
                    |                |
                    |   Stream data  |
                    | < - - - - - - -|  
                    |                |              
                    ‚ñº                |
             Audio Output üîà         |
                    |<---------------|  Terminate Thread
```

## üõ†Ô∏è Setup & Usage
1. Install Dependencies: It is recommmended to setup a virtual environment and pip install `requirements.txt`
2. Configure AWS: Ensure you have valid AWS credentials defined in the environment and access to the required services (Bedrock, Polly,Transcribe). 
```shell
export AWS_ACCESS_KEY_ID=<...>
export AWS_SECRET_ACCESS_KEY=<...>
export AWS_DEFAULT_REGION=<...> # Optional, defaults to us-east-1
```
3. Start the Application: Run the main script to start the conversational pipeline. Once running, the system will continuously listen for audio input, process it, and provide spoken responses powered by the AI model.
```shell
sudo -E python3 main.py
```
Note: sudo privileges are required to detect interrupts from the keyboard. If you don't want to use sudo, you can run the app without it, though interrupt detection may not work.